<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Arming" />
    <title>3 Logic Regression and Regularization</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Arming" type="application/atom+xml" />
    <link rel="stylesheet" href="/media/css/style.css" />
    <link rel="stylesheet" href="/media/css/highlight.css" />
    <script type="text/javascript" src="/media/js/jquery-1.7.1.min.js"></script>
    <!-- mathjax -->
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script>
         MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'" 
                }
            },
            showProcessingMessages: false
        });
    </script> 
  </head>
  <body>
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>3 Logic Regression and Regularization</h1>
        </header>
        <nav>
        <span><a title="网站首页" class="" href="/">首页</a></span>
        <span><a title="文章分类" class="" href="/categories/">分类</a></span>
        <span><a title="标签索引" class="" href="/tags/">标签</a></span>
        <!--<span><a title="友情链接" class="" href="/links/">链接</a></span>-->
        <span><a title="留言交流" class="" href="/guestbook/">留言</a></span>
        <span><a title="关于站长" class="" href="/about/">关于</a></span>
        <span><a title="种子订阅" class="" href="/feed/" target="_blank">订阅</a></span>
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2013-05-13">2013-05-13</time>
</span>

 | 
<span class="categories">
  分类
  
  <a href="/categories/#course" title="course">course</a>&nbsp;
  
</span>


 | 
<span class="tags">
  标签
  
  <a href="/tags/#ml" title="ml">ml</a>&nbsp;
  
</span>

</section>
<section class="post">
<h1 id="logistic-regression">Logistic Regression</h1>

<h2 id="classification">Classification</h2>

<script type="math/tex; mode=display">y \in {0, 1}</script>

<ul>
  <li>0: “Negative Class”</li>
  <li>1: “Positive Class”</li>
</ul>

<p>Threshold classifier output <script type="math/tex">h_\theta(x)</script> at o.5:</p>

<ul>
  <li>if <script type="math/tex">h_\theta(x) \geq 0.5</script>, predict “y = 1”</li>
  <li>if <script type="math/tex">% <![CDATA[
h_\theta(x) < 0.5 %]]></script>, predict “y = 0”</li>
</ul>

<p>Classification:  <strong>y = 0 or 1</strong></p>

<p><script type="math/tex">h_\theta(x)</script> can be &gt; 1 or &lt; 0</p>

<p>Logistic Regression: <script type="math/tex">0 \leq h_\theta(x) \leq 1</script></p>

<h2 id="hypothesis-representation">Hypothesis Representation</h2>

<p><strong>Logistic Regresssion Model</strong></p>

<p>Want <script type="math/tex">0 \leq h_\theta(x) \leq 1</script></p>

<script type="math/tex; mode=display">h_\theta(x) = g(\theta^Tx) \\
g(z)=\frac{1}{1+e^{-z}} \\
z = \theta^Tx</script>

<blockquote>
  <p><code>g(z)</code>always lies between 0 and 1.</p>
</blockquote>

<p><code>g(z)</code> is called <em>Sigmoid function</em> or <em>Logistic function</em></p>

<h2 id="descision-boundary">Descision Boundary</h2>

<ul>
  <li>Predict “y=1” if <script type="math/tex">\theta^T \geq 0</script> or <script type="math/tex">h_\theta(x) \geq 0.5</script></li>
  <li>Predict “y=0” if <script type="math/tex">% <![CDATA[
\theta^T < 0 %]]></script> or <script type="math/tex">% <![CDATA[
h_\theta(x) < 0.5 %]]></script></li>
</ul>

<h2 id="cost-function">Cost Function</h2>

<p>Training examples:</p>

<script type="math/tex; mode=display">\left [ 
\begin{array}{rrrr}
x_0 \\
x_1 \\
\ldots \\
x_n
\end{array}
 \right ],

x_0 = 1, y \in\{ 0, 1\}</script>

<p><strong>Logistic regression Cost function</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
Cost(h_\theta(x),y) =\left\{
\begin{array}{rcl}
-\log (h_\theta(x)) & \mbox{if} & y = 1 \\
-\log (1 - h_\theta(x)) & \mbox{if} & y = 0
\end{array}
\right. %]]></script>

<blockquote>
  <p><strong>Notice:</strong> In the formula above, the <em>dot</em> in the end can’t be omitted!!!</p>
</blockquote>

<blockquote>
  <p>The hypothesis will now be more accurate (or at least just as accurate) with new features, so the cost function will decrease.</p>
</blockquote>

<blockquote>
  <p>The cost function J(θ) is guaranteed to be convex for logistic regression.</p>
</blockquote>

<h2 id="simplifiled-cost-function-and-gradient-descent">Simplifiled Cost Function and Gradient Descent</h2>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})</script>

<p>The <script type="math/tex">Cost(h_\theta(x),y)</script> Can be written in the below form:</p>

<script type="math/tex; mode=display">Cost(h_\theta(x),y) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
J(\theta) &=& \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})  \nonumber \\
          &=& -\frac{1}{m} \sum_{i=1}^{m} \left [ y^{(i)}\log(h_\theta(x^{(i)})) - (1-y^{(i)})\log(1-h_\theta(x^{(i)}))  \right ]

\end{eqnarray} %]]></script>

<p>Want <script type="math/tex">min_\theta J(\theta)</script>:</p>

<p><strong>Gradient Descent:</strong></p>

<p>Repeat {</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
 \theta_j &:=& \theta_j - \alpha\frac{\partial}{\partial\theta_j} J(\theta)     \nonumber \\
   &:=& \theta_j - \alpha\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\end{eqnarray} %]]></script>

<p>}</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_j} J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}</script>

<h2 id="advanced-optimization">Advanced Optimization</h2>

<div class="highlight"><pre><code class="language-octave" data-lang="octave"><a name="True-1"></a><span class="n">options</span> <span class="p">=</span> <span class="nb">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="s">&#39;100&#39;</span><span class="p">);</span>
<a name="True-2"></a><span class="n">initialTheta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<a name="True-3"></a><span class="p">[</span><span class="n">optTheta</span><span class="p">,</span> <span class="n">functionVal</span><span class="p">,</span> <span class="n">exitFlag</span><span class="p">]</span> <span class="p">...</span>
<a name="True-4"></a>    <span class="p">=</span> <span class="n">fminunc</span><span class="p">(@</span><span class="n">costFunction</span><span class="p">,</span> <span class="n">initialTheta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
<a name="True-5"></a>
<a name="True-6"></a><span class="n">funciton</span> <span class="p">[</span><span class="n">jVal</span><span class="p">,</span> <span class="nb">gradient</span><span class="p">]</span> <span class="p">=</span> <span class="n">costFunction</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<a name="True-7"></a>    <span class="n">jVal</span> <span class="p">=</span> <span class="p">[</span><span class="n">code</span> <span class="n">to</span> <span class="n">comput</span> <span class="n">J</span><span class="p">(</span>θ<span class="p">)];</span>
<a name="True-8"></a>    <span class="nb">gradient</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">=</span> <span class="p">[</span><span class="n">code</span> <span class="n">to</span> <span class="n">compute</span> ∂<span class="o">/</span>∂θ<span class="mi">1</span><span class="n">J</span><span class="p">(</span>θ<span class="p">)];</span>
<a name="True-9"></a>    <span class="p">.</span>
<a name="True-10"></a>    <span class="p">.</span>
<a name="True-11"></a>    <span class="nb">gradient</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">=</span> <span class="p">[</span><span class="n">code</span> <span class="n">to</span> <span class="n">compute</span> ∂<span class="o">/</span>∂θ<span class="n">nJ</span><span class="p">(</span>θ<span class="p">)];</span></code></pre></div>

<h2 id="multiclass-classification-one-vs-all">Multiclass Classification: One-vs-all</h2>

<p>Train a logistic regression classifier <script type="math/tex">h_\theta^{(i)}(x)</script> for each class <script type="math/tex">i</script> to predict the probability that <script type="math/tex">y = i</script> .</p>

<script type="math/tex; mode=display">h_\theta^{(i)}(x) = P(y=i \vert x;\theta) \ (i=1,2,3,...)</script>

<h1 id="regularization">Regularization</h1>

<h2 id="the-problem-of-overfitting">The problem of Overfitting</h2>

<p><strong>Overfitting:</strong> If we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples.</p>

<p><strong>Addressing overfitting:</strong></p>

<p>Options:</p>

<ol>
  <li>Reduce number of features.
    <ul>
      <li>Manually select which features to keep</li>
      <li>Model selection algorithm</li>
    </ul>
  </li>
  <li>Regularization
    <ul>
      <li>Keep all the features, but reduce magnitude/values of parameters <script type="math/tex">\theta_j</script></li>
      <li>Works well when we have a lot of features, each of which contributes a bit to predicting y.</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>By adding a new feature, our model must be more (or just as) expressive, thus allowing it learn more complex hypotheses to fit the training set.</p>
</blockquote>

<blockquote>
  <p>Adding many new features gives us more expressive models which are able to better fit our training set. If too many new features are added, this can lead to overfitting of the training set.</p>
</blockquote>

<h2 id="cost-funciton">Cost Funciton</h2>

<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\left [ \sum_{i=1}^{m}\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2 + \lambda\sum_{j=1}^{n}\theta_j^2 \right ]</script>

<h2 id="regularized-linear-regression">Regularized Linear Regression</h2>

<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\left [ \sum_{i=1}^{m}\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2 + \lambda\sum_{j=1}^{n}\theta_j^2 \right ]</script>

<p>Gradient descent:</p>

<p>Repeat {</p>

<script type="math/tex; mode=display">\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_\theta(x^{(i)}) - y^{(i)} \right )x_0^{(i)}  \\

\theta_j := \theta_j - \alpha \left[\frac{1}{m}\sum_{i=1}^{m}\left(h_\theta(x^{(i)}) - y^{(i)} \right )x_j^{(i)} -\frac{\lambda}{m}\theta_j \right ] \ (j=1,2,3\ldots)</script>

<p>}</p>

<script type="math/tex; mode=display">\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha \frac{1}{m}\sum_{i=1}^{m}\left(h_\theta(x^{(i)}) - y^{(i)} \right )x_j^{(i)} \ (j=1,2,3\ldots)</script>

<h2 id="regularized-logistic-regression">Regularized Logistic Regression</h2>

<script type="math/tex; mode=display">J(\theta)=-\frac{1}{m} \sum_{i=1}^{m} \left [ y^{(i)}\log(h_\theta(x^{(i)})) - (1-y^{(i)})\log(1-h_\theta(x^{(i)}))  \right ]</script>

<p>Gradient descent:</p>

<p>Repeat {</p>

<script type="math/tex; mode=display">\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_\theta(x^{(i)}) - y^{(i)} \right )x_0^{(i)}  \\

\theta_j := \theta_j - \alpha \left[\frac{1}{m}\sum_{i=1}^{m}\left(h_\theta(x^{(i)}) - y^{(i)} \right )x_j^{(i)} -\frac{\lambda}{m}\theta_j \right ] \ (j=1,2,3\ldots)</script>

<p>}</p>

</section>
<section align="right">
<br/>
<span>
	<a  href="/2013/05/11/useful-linux-commands/" class="pageNav"  >上一篇</a>
	&nbsp;&nbsp;&nbsp;
	<a  href="/2013/05/15/linux-lab1-how-computer-works/" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<div class="ds-thread" />
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"mrmign"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


        </article>
      </div>

    <footer>
        <p><small>
Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> @ GitHub
             | <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/cn/" target="_blank" title="许可协议">©</a> 2012 - 2016 <a href="/about/">Arming</a>


             | <script src="http://s85.cnzz.com/stat.php?id=1000366411"></script>

         </small></p>
    </footer>

    </div>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-49976813-1', 'mrmign.github.io');
      ga('send', 'pageview');

    </script>
  </body>
</html>
